{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a50965b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03dc77c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e98396cb73845d784d1a121c4dd72d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "#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\n",
    "#https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb#scrollTo=77d9f0c5-8607-4642-a8ac-c3ab2e223ea6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d2cf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing tensorflow==2.14.0...\n",
      "Successfully installed tensorflow==2.14.0\n",
      "Installing transformers...\n",
      "Successfully installed transformers\n",
      "Installing datasets[audio]...\n",
      "Successfully installed datasets[audio]\n",
      "Installing array-api-compat...\n",
      "Successfully installed array-api-compat\n",
      "Installing accelerate...\n",
      "Successfully installed accelerate\n",
      "Installing evaluate...\n",
      "Successfully installed evaluate\n",
      "Installing jiwer...\n",
      "Successfully installed jiwer\n",
      "Installing tensorboard...\n",
      "Successfully installed tensorboard\n",
      "Installing bottleneck...\n",
      "Successfully installed bottleneck\n",
      "Installing pandas...\n",
      "Successfully installed pandas\n",
      "Installing numexpr...\n",
      "Successfully installed numexpr\n",
      "Installing numpy==1.23.5...\n",
      "Successfully installed numpy==1.23.5\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade datasets\n",
    "# !pip install --force-reinstall -v \"numpy==1.25.2\"\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# List of required packages with compatible versions\n",
    "packages = [\n",
    "    'tensorflow==2.14.0',  # Specific version for compatibility with transformers\n",
    "    'transformers',  # Latest version of transformers\n",
    "    'datasets[audio]',\n",
    "    'array-api-compat',\n",
    "    'accelerate',\n",
    "    'evaluate',\n",
    "    'jiwer',\n",
    "    'tensorboard',\n",
    "    'bottleneck',\n",
    "    'pandas',\n",
    "    'numexpr',  # Optional for pandas optimizations\n",
    "    'numpy==1.23.5',  # Ensure compatibility with transformers\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', package])\n",
    "        print(f\"Successfully installed {package}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error installing {package}: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error installing {package}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c459054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.3 | packaged by Anaconda, Inc. | (main, Apr 19 2023, 23:46:34) [MSC v.1916 64 bit (AMD64)]\n",
      "\n",
      "Library versions:\n",
      "--------------------------------------------------\n",
      "accelerate           1.1.1\n",
      "array-api-compat     1.9.1\n",
      "datasets             3.1.0\n",
      "evaluate             0.4.3\n",
      "gradio               Not installed\n",
      "huggingface_hub      0.24.6\n",
      "jiwer                3.0.5\n",
      "librosa              0.10.0.post2\n",
      "numpy                1.23.5\n",
      "soundfile            0.12.1\n",
      "tensorboard          2.18.0\n",
      "tensorflow           2.14.0\n",
      "tf-keras             Not installed\n",
      "torch                2.5.1\n",
      "tqdm                 4.67.0\n",
      "transformers         4.46.2\n",
      "\n",
      "PyTorch CUDA available: False\n",
      "\n",
      "TensorFlow GPU available: []\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pkg_resources\n",
    "\n",
    "def check_library_versions():\n",
    "    # List of all libraries used in the notebook\n",
    "    libraries = [\n",
    "        'numpy',\n",
    "        'tensorflow',\n",
    "        'tf-keras',\n",
    "        'torch',\n",
    "        'transformers',\n",
    "        'datasets',\n",
    "        'huggingface_hub',\n",
    "        'accelerate',\n",
    "        'evaluate',\n",
    "        'jiwer',\n",
    "        'tensorboard',\n",
    "        'gradio',\n",
    "        'librosa',\n",
    "        'soundfile',\n",
    "        'array-api-compat',\n",
    "        'tqdm'\n",
    "    ]\n",
    "    \n",
    "    print(\"Python version:\", sys.version)\n",
    "    print(\"\\nLibrary versions:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for lib in sorted(libraries):\n",
    "        try:\n",
    "            # Try to get version using pkg_resources\n",
    "            version = pkg_resources.get_distribution(lib).version\n",
    "            print(f\"{lib:<20} {version}\")\n",
    "        except pkg_resources.DistributionNotFound:\n",
    "            try:\n",
    "                # Try importing and getting __version__\n",
    "                module = __import__(lib)\n",
    "                version = getattr(module, '__version__', 'Version not found')\n",
    "                print(f\"{lib:<20} {version}\")\n",
    "            except ImportError:\n",
    "                print(f\"{lib:<20} Not installed\")\n",
    "        except Exception as e:\n",
    "            print(f\"{lib:<20} Error: {str(e)}\")\n",
    "    \n",
    "    # Check CUDA availability if PyTorch is installed\n",
    "    try:\n",
    "        import torch\n",
    "        print(\"\\nPyTorch CUDA available:\", torch.cuda.is_available())\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"CUDA version:\", torch.version.cuda)\n",
    "    except ImportError:\n",
    "        print(\"\\nPyTorch not installed\")\n",
    "\n",
    "    # Check TensorFlow GPU\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        print(\"\\nTensorFlow GPU available:\", tf.config.list_physical_devices('GPU'))\n",
    "    except ImportError:\n",
    "        print(\"\\nTensorFlow not installed\")\n",
    "\n",
    "check_library_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9278665a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 6540\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 2894\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# common_voice = DatasetDict()\n",
    "\n",
    "# common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"train+validation\")\n",
    "# common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"test\")\n",
    "# print(common_voice)\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "# Add error handling for better debugging\n",
    "try:\n",
    "    common_voice[\"train\"] = load_dataset(\n",
    "        \"mozilla-foundation/common_voice_11_0\", \n",
    "        \"hi\", \n",
    "        split=\"train+validation\",\n",
    "        trust_remote_code=True\n",
    "#         use_auth_token=True\n",
    "    )\n",
    "    \n",
    "    common_voice[\"test\"] = load_dataset(\n",
    "        \"mozilla-foundation/common_voice_11_0\", \n",
    "        \"hi\", \n",
    "        split=\"test\",\n",
    "        trust_remote_code=True\n",
    "#         use_auth_token=True\n",
    "    )\n",
    "    \n",
    "    print(common_voice)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    print(f\"Error message: {str(e)}\")\n",
    "    \n",
    "    # Check installed packages\n",
    "    import pkg_resources\n",
    "    installed_packages = [pkg.key for pkg in pkg_resources.working_set]\n",
    "    print(\"\\nInstalled packages:\")\n",
    "    for pkg in ['datasets', 'numpy', 'soundfile', 'librosa', 'array-api-compat']:\n",
    "        if pkg in installed_packages:\n",
    "            print(f\"{pkg}: ‚úì\")\n",
    "        else:\n",
    "            print(f\"{pkg}: ‚úó\")\n",
    "            \n",
    "# print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f36496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 6540\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 2894\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d7bfc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94487424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Hindi\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69506adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 ‡§π‡§Æ‡§®‡•á ‡§â‡§∏‡§ï‡§æ ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§Æ‡§®‡§æ‡§Ø‡§æ‡•§\n",
      "labels:    [50258, 50276, 50359, 50363, 44500, 48521, 35082, 21981, 8485, 231, 45938, 41858, 17937, 8485, 250, 35082, 27099, 48521, 3941, 99, 33279, 35082, 48449, 35082, 17937, 48268, 17937, 8703, 97, 50257]\n",
      "Decoded w/ special:    <|startoftranscript|><|hi|><|transcribe|><|notimestamps|>‡§π‡§Æ‡§®‡•á ‡§â‡§∏‡§ï‡§æ ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§Æ‡§®‡§æ‡§Ø‡§æ‡•§<|endoftext|>\n",
      "Decoded w/out special: ‡§π‡§Æ‡§®‡•á ‡§â‡§∏‡§ï‡§æ ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§Æ‡§®‡§æ‡§Ø‡§æ‡•§\n",
      "Are equal:             True\n"
     ]
    }
   ],
   "source": [
    "input_str = common_voice[\"train\"][0][\"sentence\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"labels:    {labels}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fddef165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Hindi\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00efd744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'C:\\\\Users\\\\USER\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\1af8f11ae19fe2e6693b03b87c8e509210925fde72f901466b0ad37bd45bf0c2\\\\hi_train_0/common_voice_hi_26008353.mp3', 'array': array([ 6.46234854e-26, -1.35709319e-25, -8.07793567e-26, ...,\n",
      "        1.06425944e-07,  4.46417090e-08,  2.61451660e-09]), 'sampling_rate': 48000}, 'sentence': '‡§π‡§Æ‡§®‡•á ‡§â‡§∏‡§ï‡§æ ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§Æ‡§®‡§æ‡§Ø‡§æ‡•§'}\n",
      "{'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None), 'sentence': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])\n",
    "print(common_voice[\"train\"].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dbe1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bcb0869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'C:\\\\Users\\\\USER\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\1af8f11ae19fe2e6693b03b87c8e509210925fde72f901466b0ad37bd45bf0c2\\\\hi_train_0/common_voice_hi_26008353.mp3', 'array': array([ 5.98479599e-17,  3.12250226e-17, -1.04083409e-17, ...,\n",
      "       -1.31181878e-07,  2.62807589e-07,  4.76284185e-08]), 'sampling_rate': 16000}, 'sentence': '‡§π‡§Æ‡§®‡•á ‡§â‡§∏‡§ï‡§æ ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§Æ‡§®‡§æ‡§Ø‡§æ‡•§'}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19a0d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(batch):\n",
    "#     # load and resample audio data from 48 to 16kHz\n",
    "#     audio = batch[\"audio\"]\n",
    "\n",
    "#     # compute log-Mel input features from input audio array\n",
    "#     batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "#     # encode target text to label ids\n",
    "#     batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "#     return batch\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    if audio is None or \"array\" not in audio:\n",
    "        print(\"Skipping a batch due to missing audio data.\")\n",
    "        return {\"input_features\": [], \"labels\": []}  # Skip this batch\n",
    "\n",
    "    try:\n",
    "        input_features = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"])\n",
    "        batch[\"input_features\"] = input_features.input_features[0]\n",
    "        batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "        return batch\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "460414ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': [{'path': 'C:\\\\Users\\\\USER\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\1af8f11ae19fe2e6693b03b87c8e509210925fde72f901466b0ad37bd45bf0c2\\\\hi_train_0/common_voice_hi_26008353.mp3', 'array': array([ 5.98479599e-17,  3.12250226e-17, -1.04083409e-17, ...,\n",
      "       -1.31181878e-07,  2.62807589e-07,  4.76284185e-08]), 'sampling_rate': 16000}, {'path': 'C:\\\\Users\\\\USER\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\1af8f11ae19fe2e6693b03b87c8e509210925fde72f901466b0ad37bd45bf0c2\\\\hi_train_0/common_voice_hi_26010468.mp3', 'array': array([ 2.91038305e-11,  5.82076609e-11,  2.91038305e-11, ...,\n",
      "        7.53775851e-08, -9.65526965e-08, -5.93558980e-09]), 'sampling_rate': 16000}, {'path': 'C:\\\\Users\\\\USER\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\1af8f11ae19fe2e6693b03b87c8e509210925fde72f901466b0ad37bd45bf0c2\\\\hi_train_0/common_voice_hi_26010469.mp3', 'array': array([ 1.81898940e-11,  1.20053301e-10, -2.03726813e-10, ...,\n",
      "        7.85149723e-13,  1.49213975e-13,  2.15294449e-12]), 'sampling_rate': 16000}, {'path': 'C:\\\\Users\\\\USER\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\1af8f11ae19fe2e6693b03b87c8e509210925fde72f901466b0ad37bd45bf0c2\\\\hi_train_0/common_voice_hi_26010470.mp3', 'array': array([-2.91038305e-10,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "        1.74622983e-10,  1.16415322e-10,  1.74622983e-10]), 'sampling_rate': 16000}, {'path': 'C:\\\\Users\\\\USER\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\1af8f11ae19fe2e6693b03b87c8e509210925fde72f901466b0ad37bd45bf0c2\\\\hi_train_0/common_voice_hi_26010471.mp3', 'array': array([-8.73114914e-11, -1.16415322e-10, -1.30967237e-10, ...,\n",
      "       -4.77302819e-09, -1.76951289e-08, -3.84170562e-09]), 'sampling_rate': 16000}], 'sentence': ['‡§π‡§Æ‡§®‡•á ‡§â‡§∏‡§ï‡§æ ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§Æ‡§®‡§æ‡§Ø‡§æ‡•§', '‡§∏‡§æ‡§â‡§• ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§®‡§ó‡§∞ ‡§®‡§ø‡§ó‡§Æ ‡§∏‡§ñ‡•ç‡§§, ‡§∂‡•â‡§™‡§ø‡§Ç‡§ó ‡§Æ‡•â‡§≤ ‡§ï‡•á ‡§¨‡§æ‡§π‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§¶‡§ø‡§ñ‡•á‡§Ç‡§ó‡•á ‡§π‡•ã‡§∞‡•ç‡§°‡§ø‡§Ç‡§ó', '‡§â‡§§‡•ç‡§§‡§∞ ‡§ï‡•ã‡§∞‡§ø‡§Ø‡§æ ‡§®‡•á ‡§Ö‡§Æ‡•á‡§∞‡§ø‡§ï‡§æ ‡§ï‡•ã ‡§¶‡•Ä ‡§π‡§Æ‡§≤‡•á ‡§ï‡•Ä ‡§ß‡§Æ‡§ï‡•Ä', '‡§Ö‡§ó‡§≤‡•á ‡§ï‡§Æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§Ö‡§®‡•á‡§ï ‡§∞‡•ã‡§Æ‡§® ‡§Æ‡•Ç‡§∞‡•ç‡§§‡§ø‡§Ø‡§æ‡§Å ‡§π‡•à‡§Ç‡•§', '‡§§‡•Å‡§Æ ‡§®‡•á ‡§ü‡•â‡§Æ ‡§ï‡•ã ‡§ï‡§π‡§æ‡§Å ‡§≠‡•á‡§ú ‡§¶‡§ø‡§Ø‡§æ?']}\n",
      "['audio', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "# Inspect a few samples before processing\n",
    "print(common_voice[\"train\"][:5])\n",
    "print(common_voice[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cdf7746",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1)\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)\n",
    "\n",
    "# Use num_proc=1 for debugging\n",
    "# common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1)\n",
    "# Step 2: Select the first 100 rows\n",
    "# common_voice = common_voice.select(range(100))  # Adjust range to select 100 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f365f3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 6540\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 2894\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(common_voice)\n",
    "# common_voice = DatasetDict({\n",
    "#     \"train\": common_voice[\"train\"].select(range(100)),  # Select the first 100 rows from train\n",
    "#     \"test\": common_voice[\"test\"].select(range(20))  # Select the first 20 rows from test\n",
    "# })\n",
    "# print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8875d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c65102d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.language = \"hindi\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3dfc0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3139351",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85bc71e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jiwer\n",
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a155e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4a3fa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2bf734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorboard\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f34de68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8d8e252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-hi\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "# from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# # Define training arguments\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./whisper-small-hi\",  # change to a repo name of your choice\n",
    "#     per_device_train_batch_size=16,\n",
    "#     gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "#     learning_rate=1e-5,\n",
    "#     warmup_steps=500,\n",
    "#     max_steps=4000,\n",
    "#     gradient_checkpointing=True,\n",
    "#     fp16=True,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     predict_with_generate=True,\n",
    "#     generation_max_length=225,\n",
    "#     save_steps=1000,\n",
    "#     eval_steps=1000,\n",
    "#     logging_steps=25,\n",
    "#     report_to=[\"tensorboard\"],\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"wer\",  # Word error rate (WER) metric for best model selection\n",
    "#     greater_is_better=False,      # Lower WER is better\n",
    "#     push_to_hub=True,             # To upload to Hugging Face hub\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbe100d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.15.1 tf-keras==2.15.1 transformers==4.30.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a50efc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_1648\\3035936458.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "# from transformers import Seq2SeqTrainer\n",
    "\n",
    "# # Set up the trainer\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     args=training_args,\n",
    "#     model=model,\n",
    "#     train_dataset=common_voice[\"train\"],\n",
    "#     eval_dataset=common_voice[\"test\"],\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     tokenizer=processor.tokenizer,  # Pass the correct tokenizer here\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef14eb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c363c737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/4000 06:01 < 401:32:27, 0.00 it/s, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e299773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kwargs = {\n",
    "#     \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
    "#     \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
    "#     \"dataset_args\": \"config: hi, split: test\",\n",
    "#     \"language\": \"hi\",\n",
    "#     \"model_name\": \"Whisper Small Hi - ASR_WHISPER_MODEL\",  # a 'pretty' name for your model\n",
    "#     \"finetuned_from\": \"openai/whisper-small\",\n",
    "#     \"tasks\": \"automatic-speech-recognition\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b770a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# import gradio as gr\n",
    "\n",
    "# pipe = pipeline(model=\"sanchit-gandhi/whisper-small-hi\")  # change to \"your-username/the-name-you-picked\"\n",
    "\n",
    "# def transcribe(audio):\n",
    "#     text = pipe(audio)[\"text\"]\n",
    "#     return text\n",
    "\n",
    "# iface = gr.Interface(\n",
    "#     fn=transcribe,\n",
    "#     inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
    "#     outputs=\"text\",\n",
    "#     title=\"Whisper Small Hindi\",\n",
    "#     description=\"Realtime demo for Hindi speech recognition using a fine-tuned Whisper small model.\",\n",
    "# )\n",
    "\n",
    "# iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c017bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(\"sanchit-gandhi/whisper-small-hi\")\n",
    "# processor = WhisperProcessor.from_pretrained(\"sanchit-gandhi/whisper-small-hi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
